{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25113c97",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "197663bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "from collections import Counter\n",
    "\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f4373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Provider: openai\n",
      "LLM Model: gpt-3.5-turbo\n",
      "Temperature: 0.2\n",
      "Max Tokens: 120\n",
      "API Key configured: ✓ Yes\n"
     ]
    }
   ],
   "source": [
    "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"openai\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", \"\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-3.5-turbo\")\n",
    "\n",
    "LLM_TEMPERATURE = 0.2\n",
    "LLM_MAX_TOKENS = 120\n",
    "\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "print(f\"LLM Model: {LLM_MODEL}\")\n",
    "print(f\"Temperature: {LLM_TEMPERATURE}\")\n",
    "print(f\"Max Tokens: {LLM_MAX_TOKENS}\")\n",
    "print(f\"API Key configured: {'✓ Yes' if LLM_API_KEY else '✗ No (set LLM_API_KEY env var)'}\")\n",
    "\n",
    "if not LLM_API_KEY:\n",
    "    print(\"\\n⚠ WARNING: LLM_API_KEY not set. Set environment variable before running evaluation.\")\n",
    "    print(\"Example: export LLM_API_KEY=sk-...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912e6ed",
   "metadata": {},
   "source": [
    "## 2. Load Yelp Dataset\n",
    "\n",
    "**Dataset:** Yelp Reviews from Kaggle  \n",
    "**Download:** https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\n",
    "\n",
    "Place the `yelp.csv` file in `notebooks/data/yelp.csv` relative to the repo root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/harshkanani/Desktop/fyndAssignment/notebooks/data/yelp.csv\n",
      "Output directory: /Users/harshkanani/Desktop/fyndAssignment/notebooks/outputs\n"
     ]
    }
   ],
   "source": [
    "YELP_CSV_PATH = os.getenv(\"YELP_CSV_PATH\", \"/Users/harshkanani/Desktop/fyndAssignment/notebooks/data/yelp.csv\")\n",
    "\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset path: {YELP_CSV_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f04f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 10000 rows from /Users/harshkanani/Desktop/fyndAssignment/notebooks/data/yelp.csv\n",
      "  Columns: ['business_id', 'date', 'review_id', 'stars', 'text', 'type', 'user_id', 'cool', 'useful', 'funny']\n",
      "  After cleaning: 10000 valid rows\n",
      "  Star distribution:\n",
      "stars\n",
      "1     749\n",
      "2     927\n",
      "3    1461\n",
      "4    3526\n",
      "5    3337\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_yelp_dataset(csv_path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load Yelp reviews dataset from CSV.\n",
    "    Handles column name variations robustly.\n",
    "    Returns DataFrame with columns: stars, text\n",
    "    \"\"\"\n",
    "    path = Path(csv_path)\n",
    "    \n",
    "    if not path.exists():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ERROR: Dataset file not found!\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nExpected location: {path.absolute()}\")\n",
    "        print(\"\\nTo fix this:\")\n",
    "        print(\"1. Download the Yelp Reviews Dataset from Kaggle:\")\n",
    "        print(\"   https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\")\n",
    "        print(f\"2. Place the yelp.csv file at: {path}\")\n",
    "        print(\"3. Or set YELP_CSV_PATH environment variable to your file location.\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"✓ Loaded {len(df)} rows from {path}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        star_col = None\n",
    "        for col in df.columns:\n",
    "            if col.lower() in ['stars', 'star', 'rating', 'ratings', 'score']:\n",
    "                star_col = col\n",
    "                break\n",
    "        \n",
    "        text_col = None\n",
    "        for col in df.columns:\n",
    "            if col.lower() in ['text', 'review', 'review_text', 'content', 'body']:\n",
    "                text_col = col\n",
    "                break\n",
    "        \n",
    "        if not star_col or not text_col:\n",
    "            print(f\"\\n✗ Could not identify required columns.\")\n",
    "            print(f\"  Found columns: {list(df.columns)}\")\n",
    "            print(f\"  Need: a star/rating column and a text/review column\")\n",
    "            return None\n",
    "        \n",
    "        df = df.rename(columns={star_col: 'stars', text_col: 'text'})\n",
    "        df = df[['stars', 'text']].dropna()\n",
    "        \n",
    "        df['stars'] = df['stars'].astype(int)\n",
    "        df = df[df['stars'].between(1, 5)]\n",
    "        \n",
    "        print(f\"  After cleaning: {len(df)} valid rows\")\n",
    "        print(f\"  Star distribution:\\n{df['stars'].value_counts().sort_index()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df_full = load_yelp_dataset(YELP_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b89aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Sampled 500 reviews (stratified by star rating)\n",
      "Star distribution in sample:\n",
      "stars\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 3 samples:\n",
      "  [4★] Went on Halloween night for dinner.  Very good.  The steak came out sizzling in ...\n",
      "  [1★] This place has really bad service and the food is barely decent, I would advise ...\n",
      "  [4★] Arizona definitely lack cool spots, but luckily they have The Vig. The venue has...\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 500\n",
    "\n",
    "if df_full is not None:\n",
    "    samples_per_star = SAMPLE_SIZE // 5\n",
    "    \n",
    "    sampled_dfs = []\n",
    "    for star in range(1, 6):\n",
    "        star_df = df_full[df_full['stars'] == star]\n",
    "        n_samples = min(samples_per_star, len(star_df))\n",
    "        if n_samples > 0:\n",
    "            sampled_dfs.append(star_df.sample(n=n_samples, random_state=RANDOM_SEED))\n",
    "    \n",
    "    df_sample = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    df_sample = df_sample.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n✓ Sampled {len(df_sample)} reviews (stratified by star rating)\")\n",
    "    print(f\"Star distribution in sample:\\n{df_sample['stars'].value_counts().sort_index()}\")\n",
    "    print(f\"\\nFirst 3 samples:\")\n",
    "    for i, row in df_sample.head(3).iterrows():\n",
    "        print(f\"  [{row['stars']}★] {row['text'][:80]}...\")\n",
    "else:\n",
    "    df_sample = None\n",
    "    print(\"\\n⚠ Cannot sample - dataset not loaded. See instructions above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted 5 few-shot examples (1 per star rating)\n",
      "  [1★] DON'T GET TIFFANY UNLESS YOU WANT A SPEEDY EXPERIENCE\n",
      "Pretty...\n",
      "  [2★] Food was marginal....Sunday brunch.  Prices were stupid.  An...\n",
      "  [3★] I truly wanted to love this place! I had the shrimp/pork spr...\n",
      "  [4★] Not a bad visit. I regularly visit the Location in Columbus,...\n",
      "  [5★] RIP AZ Coffee Connection.  :(  I stopped by two days ago una...\n"
     ]
    }
   ],
   "source": [
    "FEW_SHOT_EXAMPLES = []\n",
    "\n",
    "if df_full is not None:\n",
    "    if df_sample is not None:\n",
    "        sample_indices = set(df_sample.index) if 'index' in df_sample.columns else set()\n",
    "        for star in range(1, 6):\n",
    "            star_df = df_full[df_full['stars'] == star]\n",
    "            example_row = star_df.sample(n=1, random_state=RANDOM_SEED + 100 + star).iloc[0]\n",
    "            FEW_SHOT_EXAMPLES.append({\n",
    "                'stars': int(example_row['stars']),\n",
    "                'text': example_row['text'][:300]\n",
    "            })\n",
    "    \n",
    "    print(f\"✓ Extracted {len(FEW_SHOT_EXAMPLES)} few-shot examples (1 per star rating)\")\n",
    "    for ex in FEW_SHOT_EXAMPLES:\n",
    "        print(f\"  [{ex['stars']}★] {ex['text'][:60]}...\")\n",
    "else:\n",
    "    print(\"⚠ Cannot extract few-shot examples - dataset not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853bd88",
   "metadata": {},
   "source": [
    "## 3. Define Prompting Approaches\n",
    "\n",
    "All approaches must output the **exact same JSON schema**:\n",
    "```json\n",
    "{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a571a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Approach 1 (Zero-Shot Rubric) defined\n"
     ]
    }
   ],
   "source": [
    "def approach_1_zero_shot_rubric(review_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Zero-shot with explicit rating rubric/criteria.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a review rating predictor. Predict the star rating (1-5) for this review.\n",
    "\n",
    "Rating Criteria:\n",
    "- 5 stars: Extremely positive, enthusiastic praise, no complaints\n",
    "- 4 stars: Mostly positive with minor issues mentioned\n",
    "- 3 stars: Mixed or neutral, both positives and negatives\n",
    "- 2 stars: Mostly negative, significant complaints\n",
    "- 1 star: Extremely negative, strong dissatisfaction\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"\n",
    "{review_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Respond with ONLY this JSON (no other text):\n",
    "{{\"predicted_stars\": <1-5>, \"explanation\": \"<15 words max>\"}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(\"✓ Approach 1 (Zero-Shot Rubric) defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb11ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Approach 2 (Few-Shot Examples) defined\n"
     ]
    }
   ],
   "source": [
    "def approach_2_few_shot(review_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Few-shot with 5 real examples (one per star rating).\n",
    "    Examples are from held-out data, not evaluation sample.\n",
    "    \"\"\"\n",
    "    examples_text = \"\"\n",
    "    for ex in FEW_SHOT_EXAMPLES:\n",
    "        examples_text += f'''Review: \"{ex['text'][:200]}...\"\n",
    "Output: {{\"predicted_stars\": {ex['stars']}, \"explanation\": \"Example {ex['stars']}-star review.\"}}\n",
    "\n",
    "'''\n",
    "    \n",
    "    prompt = f\"\"\"Predict the star rating (1-5) for customer reviews. Learn from these examples:\n",
    "\n",
    "{examples_text}\n",
    "Now predict for this review:\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Respond with ONLY valid JSON:\n",
    "{{\"predicted_stars\": <1-5>, \"explanation\": \"<15 words max>\"}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(\"✓ Approach 2 (Few-Shot Examples) defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7db228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Approach 3 (Structured + Constraints) defined\n"
     ]
    }
   ],
   "source": [
    "def approach_3_structured_constraints(review_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Structured prompt with explicit constraints:\n",
    "    - JSON-only output\n",
    "    - Explanation <= 25 words\n",
    "    - Rating must be integer 1-5\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"TASK: Predict star rating for this review.\n",
    "\n",
    "REVIEW:\n",
    "{review_text}\n",
    "\n",
    "CONSTRAINTS:\n",
    "- predicted_stars: integer from 1 to 5 (inclusive)\n",
    "- explanation: maximum 25 words\n",
    "- Output ONLY the JSON object, nothing else\n",
    "\n",
    "OUTPUT FORMAT (strict):\n",
    "{{\"predicted_stars\": <int>, \"explanation\": \"<string>\"}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(\"✓ Approach 3 (Structured + Constraints) defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26a20b",
   "metadata": {},
   "source": [
    "## 4. LLM Call Wrapper\n",
    "\n",
    "Supports: OpenAI, Gemini, OpenRouter (OpenAI-compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM call wrapper ready\n",
      "  Configured for: openai / gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "def call_llm(prompt: str) -> Tuple[Optional[str], float, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Call LLM API based on configured provider.\n",
    "    \n",
    "    Returns: (response_text, latency_ms, error_message)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not LLM_API_KEY:\n",
    "        return None, 0.0, \"LLM_API_KEY not configured\"\n",
    "    \n",
    "    try:\n",
    "        if LLM_PROVIDER == \"openai\":\n",
    "            response = httpx.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\"Authorization\": f\"Bearer {LLM_API_KEY}\"},\n",
    "                json={\n",
    "                    \"model\": LLM_MODEL,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"temperature\": LLM_TEMPERATURE,\n",
    "                    \"max_tokens\": LLM_MAX_TOKENS\n",
    "                },\n",
    "                timeout=30.0\n",
    "            )\n",
    "            \n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                return None, latency, f\"API error {response.status_code}: {response.text[:100]}\"\n",
    "            \n",
    "            result = response.json()\n",
    "            text = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            return text, latency, None\n",
    "            \n",
    "        elif LLM_PROVIDER == \"gemini\":\n",
    "            response = httpx.post(\n",
    "                f\"https://generativelanguage.googleapis.com/v1beta/models/{LLM_MODEL}:generateContent\",\n",
    "                params={\"key\": LLM_API_KEY},\n",
    "                json={\n",
    "                    \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "                    \"generationConfig\": {\n",
    "                        \"temperature\": LLM_TEMPERATURE,\n",
    "                        \"maxOutputTokens\": LLM_MAX_TOKENS\n",
    "                    }\n",
    "                },\n",
    "                timeout=30.0\n",
    "            )\n",
    "            \n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                return None, latency, f\"API error {response.status_code}: {response.text[:100]}\"\n",
    "            \n",
    "            result = response.json()\n",
    "            text = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].strip()\n",
    "            return text, latency, None\n",
    "            \n",
    "        elif LLM_PROVIDER == \"openrouter\":\n",
    "            response = httpx.post(\n",
    "                \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {LLM_API_KEY}\",\n",
    "                    \"HTTP-Referer\": \"https://github.com/fynd-ai-intern\",\n",
    "                    \"X-Title\": \"Task1-Rating-Prediction\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": LLM_MODEL,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"temperature\": LLM_TEMPERATURE,\n",
    "                    \"max_tokens\": LLM_MAX_TOKENS\n",
    "                },\n",
    "                timeout=30.0\n",
    "            )\n",
    "            \n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                return None, latency, f\"API error {response.status_code}: {response.text[:100]}\"\n",
    "            \n",
    "            result = response.json()\n",
    "            text = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            return text, latency, None\n",
    "            \n",
    "        else:\n",
    "            return None, 0.0, f\"Unsupported provider: {LLM_PROVIDER}\"\n",
    "            \n",
    "    except httpx.TimeoutException:\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        return None, latency, \"Request timed out\"\n",
    "    except Exception as e:\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        return None, latency, str(e)\n",
    "\n",
    "\n",
    "print(\"✓ LLM call wrapper ready\")\n",
    "print(f\"  Configured for: {LLM_PROVIDER} / {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753d3d2",
   "metadata": {},
   "source": [
    "## 5. JSON Validation\n",
    "\n",
    "Strict schema validation - must have `predicted_stars` (int 1-5) and `explanation` (non-empty string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f70130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation tests:\n",
      "  ✓ '{\"predicted_stars\": 4, \"explanation\": \"G...' -> Valid\n",
      "  ✗ '{\"predicted_stars\": 6, \"explanation\": \"O...' -> 'predicted_stars' must be 1-5, got 6\n",
      "  ✗ '{\"rating\": 3}...' -> Missing 'predicted_stars' field\n",
      "  ✗ 'Not JSON at all...' -> Invalid JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "def validate_and_parse_response(response_text: str) -> Tuple[Optional[Dict], bool, str]:\n",
    "    \"\"\"\n",
    "    Validate LLM response against required schema.\n",
    "    \n",
    "    Required schema:\n",
    "    {\n",
    "      \"predicted_stars\": <int 1-5>,\n",
    "      \"explanation\": \"<non-empty string>\"\n",
    "    }\n",
    "    \n",
    "    Returns: (parsed_data, is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if not response_text:\n",
    "        return None, False, \"Empty response\"\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(response_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, False, f\"Invalid JSON: {str(e)[:50]}\"\n",
    "    \n",
    "    if not isinstance(data, dict):\n",
    "        return None, False, \"Response is not a JSON object\"\n",
    "    \n",
    "    if \"predicted_stars\" not in data:\n",
    "        return None, False, \"Missing 'predicted_stars' field\"\n",
    "    \n",
    "    stars = data[\"predicted_stars\"]\n",
    "    if not isinstance(stars, int):\n",
    "        try:\n",
    "            stars = int(stars)\n",
    "            data[\"predicted_stars\"] = stars\n",
    "        except (ValueError, TypeError):\n",
    "            return None, False, f\"'predicted_stars' must be integer, got {type(stars).__name__}\"\n",
    "    \n",
    "    if not (1 <= stars <= 5):\n",
    "        return None, False, f\"'predicted_stars' must be 1-5, got {stars}\"\n",
    "    \n",
    "    if \"explanation\" not in data:\n",
    "        return None, False, \"Missing 'explanation' field\"\n",
    "    \n",
    "    explanation = data[\"explanation\"]\n",
    "    if not isinstance(explanation, str) or not explanation.strip():\n",
    "        return None, False, \"'explanation' must be non-empty string\"\n",
    "    \n",
    "    return data, True, \"\"\n",
    "\n",
    "\n",
    "test_cases = [\n",
    "    '{\"predicted_stars\": 4, \"explanation\": \"Good review\"}',\n",
    "    '{\"predicted_stars\": 6, \"explanation\": \"Out of range\"}',\n",
    "    '{\"rating\": 3}',\n",
    "    'Not JSON at all',\n",
    "]\n",
    "\n",
    "print(\"Validation tests:\")\n",
    "for tc in test_cases:\n",
    "    _, valid, err = validate_and_parse_response(tc)\n",
    "    status = \"✓\" if valid else \"✗\"\n",
    "    print(f\"  {status} '{tc[:40]}...' -> {err if err else 'Valid'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d5be4",
   "metadata": {},
   "source": [
    "## 6. Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3767e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation framework ready\n"
     ]
    }
   ],
   "source": [
    "def evaluate_approach(\n",
    "    approach_name: str,\n",
    "    prompt_fn,\n",
    "    reviews_df: pd.DataFrame,\n",
    "    sample_size: int = 50\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a prompting approach on reviews.\n",
    "    \n",
    "    Returns metrics and predictions.\n",
    "    \"\"\"\n",
    "    sample = reviews_df.head(sample_size).copy()\n",
    "    predictions = []\n",
    "    latencies = []\n",
    "    \n",
    "    print(f\"\\nEvaluating '{approach_name}' on {len(sample)} samples...\")\n",
    "    \n",
    "    for idx, row in tqdm(sample.iterrows(), total=len(sample), desc=approach_name[:20]):\n",
    "        actual_stars = row['stars']\n",
    "        review_text = row['text'][:1000]\n",
    "        \n",
    "        prompt = prompt_fn(review_text)\n",
    "        \n",
    "        response_text, latency_ms, api_error = call_llm(prompt)\n",
    "        latencies.append(latency_ms)\n",
    "        \n",
    "        if api_error:\n",
    "            parsed = None\n",
    "            json_valid = False\n",
    "            validation_error = api_error\n",
    "        else:\n",
    "            parsed, json_valid, validation_error = validate_and_parse_response(response_text)\n",
    "        \n",
    "        predicted_stars = parsed[\"predicted_stars\"] if parsed else None\n",
    "        explanation = parsed[\"explanation\"] if parsed else None\n",
    "        \n",
    "        predictions.append({\n",
    "            \"actual_stars\": actual_stars,\n",
    "            \"predicted_stars\": predicted_stars,\n",
    "            \"explanation\": explanation,\n",
    "            \"json_valid\": json_valid,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"error\": validation_error if not json_valid else None,\n",
    "            \"raw_response\": response_text[:200] if response_text else None\n",
    "        })\n",
    "    \n",
    "    total = len(predictions)\n",
    "    valid_count = sum(1 for p in predictions if p[\"json_valid\"])\n",
    "    correct_count = sum(1 for p in predictions if p[\"json_valid\"] and p[\"predicted_stars\"] == p[\"actual_stars\"])\n",
    "    \n",
    "    accuracy_over_all = correct_count / total if total > 0 else 0.0\n",
    "    \n",
    "    accuracy_over_valid = correct_count / valid_count if valid_count > 0 else 0.0\n",
    "    \n",
    "    json_validity_rate = valid_count / total if total > 0 else 0.0\n",
    "    \n",
    "    avg_latency = statistics.mean(latencies) if latencies else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"approach\": approach_name,\n",
    "        \"predictions\": predictions,\n",
    "        \"metrics\": {\n",
    "            \"accuracy_over_all\": accuracy_over_all,\n",
    "            \"accuracy_over_valid\": accuracy_over_valid,\n",
    "            \"json_validity_rate\": json_validity_rate,\n",
    "            \"avg_latency_ms\": avg_latency,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"valid_count\": valid_count,\n",
    "            \"total_samples\": total\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380d617",
   "metadata": {},
   "source": [
    "## 7. Consistency/Reliability Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Consistency testing function ready\n"
     ]
    }
   ],
   "source": [
    "def test_consistency(\n",
    "    approach_name: str,\n",
    "    prompt_fn,\n",
    "    reviews_df: pd.DataFrame,\n",
    "    n_reviews: int = 30,\n",
    "    num_runs: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Test consistency by running the same prompt multiple times.\n",
    "    \n",
    "    Returns:\n",
    "    - agreement_rate: fraction where all runs match\n",
    "    - avg_std: average std-dev of valid predictions\n",
    "    - per_review results\n",
    "    \"\"\"\n",
    "    sample = reviews_df.sample(n=min(n_reviews, len(reviews_df)), random_state=RANDOM_SEED + 999)\n",
    "    \n",
    "    results = []\n",
    "    all_agree_count = 0\n",
    "    std_devs = []\n",
    "    \n",
    "    print(f\"\\nTesting consistency for '{approach_name}' ({n_reviews} reviews x {num_runs} runs)...\")\n",
    "    \n",
    "    for idx, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Consistency\"):\n",
    "        review_text = row['text'][:1000]\n",
    "        prompt = prompt_fn(review_text)\n",
    "        \n",
    "        run_predictions = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            response_text, _, api_error = call_llm(prompt)\n",
    "            \n",
    "            if not api_error:\n",
    "                parsed, valid, _ = validate_and_parse_response(response_text)\n",
    "                if valid:\n",
    "                    run_predictions.append(parsed[\"predicted_stars\"])\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        if len(run_predictions) == num_runs and len(set(run_predictions)) == 1:\n",
    "            all_agree_count += 1\n",
    "        \n",
    "        if len(run_predictions) >= 2:\n",
    "            std_devs.append(statistics.stdev(run_predictions))\n",
    "        \n",
    "        results.append({\n",
    "            \"review_preview\": review_text[:50],\n",
    "            \"predictions\": run_predictions,\n",
    "            \"all_agree\": len(run_predictions) == num_runs and len(set(run_predictions)) == 1\n",
    "        })\n",
    "    \n",
    "    agreement_rate = all_agree_count / len(sample) if sample.shape[0] > 0 else 0.0\n",
    "    avg_std = statistics.mean(std_devs) if std_devs else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"approach\": approach_name,\n",
    "        \"agreement_rate\": agreement_rate,\n",
    "        \"avg_std\": avg_std,\n",
    "        \"n_reviews\": len(sample),\n",
    "        \"num_runs\": num_runs,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Consistency testing function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3cdaf",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation\n",
    "\n",
    "Execute evaluation for all 3 approaches on the sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8af091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION: 500 samples per approach\n",
      "Provider: openai | Model: gpt-3.5-turbo\n",
      "======================================================================\n",
      "\n",
      "Evaluating 'Zero-Shot Rubric' on 500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-Shot Rubric: 100%|██████████| 500/500 [07:07<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating 'Few-Shot Examples' on 500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Few-Shot Examples: 100%|██████████| 500/500 [07:26<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating 'Structured Constraints' on 500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Structured Constrain: 100%|██████████| 500/500 [07:28<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✓ All approaches evaluated successfully\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EVAL_SAMPLE_SIZE = 500\n",
    "\n",
    "results = []\n",
    "\n",
    "if df_sample is not None and LLM_API_KEY:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION: {EVAL_SAMPLE_SIZE} samples per approach\")\n",
    "    print(f\"Provider: {LLM_PROVIDER} | Model: {LLM_MODEL}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        result1 = evaluate_approach(\n",
    "            \"Zero-Shot Rubric\",\n",
    "            approach_1_zero_shot_rubric,\n",
    "            df_sample,\n",
    "            sample_size=EVAL_SAMPLE_SIZE\n",
    "        )\n",
    "        results.append(result1)\n",
    "        \n",
    "        result2 = evaluate_approach(\n",
    "            \"Few-Shot Examples\",\n",
    "            approach_2_few_shot,\n",
    "            df_sample,\n",
    "            sample_size=EVAL_SAMPLE_SIZE\n",
    "        )\n",
    "        results.append(result2)\n",
    "        \n",
    "        result3 = evaluate_approach(\n",
    "            \"Structured Constraints\",\n",
    "            approach_3_structured_constraints,\n",
    "            df_sample,\n",
    "            sample_size=EVAL_SAMPLE_SIZE\n",
    "        )\n",
    "        results.append(result3)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"✓ All approaches evaluated successfully\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n⚠ Cannot run evaluation:\")\n",
    "    if df_sample is None:\n",
    "        print(\"  - Dataset not loaded (see instructions above)\")\n",
    "    if not LLM_API_KEY:\n",
    "        print(\"  - LLM_API_KEY not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64464c71",
   "metadata": {},
   "source": [
    "## 9. Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac0433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "COMPARISON TABLE: Rating Prediction Approaches\n",
      "==========================================================================================\n",
      "              Approach Accuracy (All) Accuracy (Valid) JSON Validity Avg Latency (ms) Correct/Valid/Total\n",
      "      Zero-Shot Rubric          61.2%            61.2%        100.0%              850         306/500/500\n",
      "     Few-Shot Examples          63.8%            63.8%        100.0%              887         319/500/500\n",
      "Structured Constraints          63.2%            63.2%        100.0%              893         316/500/500\n",
      "==========================================================================================\n",
      "\n",
      "Note:\n",
      "  - Accuracy (All): correct / total (invalid JSON counts as wrong)\n",
      "  - Accuracy (Valid): correct / valid_json_count\n",
      "  - JSON Validity: valid_json_count / total\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        m = result[\"metrics\"]\n",
    "        comparison_data.append({\n",
    "            \"Approach\": result[\"approach\"],\n",
    "            \"Accuracy (All)\": f\"{m['accuracy_over_all']:.1%}\",\n",
    "            \"Accuracy (Valid)\": f\"{m['accuracy_over_valid']:.1%}\",\n",
    "            \"JSON Validity\": f\"{m['json_validity_rate']:.1%}\",\n",
    "            \"Avg Latency (ms)\": f\"{m['avg_latency_ms']:.0f}\",\n",
    "            \"Correct/Valid/Total\": f\"{m['correct_count']}/{m['valid_count']}/{m['total_samples']}\"\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"COMPARISON TABLE: Rating Prediction Approaches\")\n",
    "    print(\"=\"*90)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\"*90)\n",
    "    print(\"\\nNote:\")\n",
    "    print(\"  - Accuracy (All): correct / total (invalid JSON counts as wrong)\")\n",
    "    print(\"  - Accuracy (Valid): correct / valid_json_count\")\n",
    "    print(\"  - JSON Validity: valid_json_count / total\")\n",
    "else:\n",
    "    print(\"\\n⚠ No results available. Run evaluation cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6aef8f",
   "metadata": {},
   "source": [
    "## 10. Consistency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONSISTENCY TESTING: 50 reviews x 3 runs each\n",
      "======================================================================\n",
      "\n",
      "Testing consistency for 'Zero-Shot Rubric' (50 reviews x 3 runs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consistency: 100%|██████████| 50/50 [02:50<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot Rubric:\n",
      "  Agreement Rate: 94.0%\n",
      "  Avg Std Dev: 0.035\n",
      "\n",
      "Testing consistency for 'Few-Shot Examples' (50 reviews x 3 runs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consistency: 100%|██████████| 50/50 [02:46<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Examples:\n",
      "  Agreement Rate: 96.0%\n",
      "  Avg Std Dev: 0.023\n",
      "\n",
      "Testing consistency for 'Structured Constraints' (50 reviews x 3 runs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consistency: 100%|██████████| 50/50 [03:04<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structured Constraints:\n",
      "  Agreement Rate: 96.0%\n",
      "  Avg Std Dev: 0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CONSISTENCY_N_REVIEWS = 50\n",
    "CONSISTENCY_NUM_RUNS = 3\n",
    "\n",
    "consistency_results = []\n",
    "\n",
    "if df_sample is not None and LLM_API_KEY and results:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CONSISTENCY TESTING: {CONSISTENCY_N_REVIEWS} reviews x {CONSISTENCY_NUM_RUNS} runs each\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    approach_fns = {\n",
    "        \"Zero-Shot Rubric\": approach_1_zero_shot_rubric,\n",
    "        \"Few-Shot Examples\": approach_2_few_shot,\n",
    "        \"Structured Constraints\": approach_3_structured_constraints\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for approach_name, prompt_fn in approach_fns.items():\n",
    "            cons_result = test_consistency(\n",
    "                approach_name,\n",
    "                prompt_fn,\n",
    "                df_sample,\n",
    "                n_reviews=CONSISTENCY_N_REVIEWS,\n",
    "                num_runs=CONSISTENCY_NUM_RUNS\n",
    "            )\n",
    "            consistency_results.append(cons_result)\n",
    "            \n",
    "            print(f\"\\n{approach_name}:\")\n",
    "            print(f\"  Agreement Rate: {cons_result['agreement_rate']:.1%}\")\n",
    "            print(f\"  Avg Std Dev: {cons_result['avg_std']:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠ Consistency testing error: {e}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Cannot run consistency tests. Ensure dataset and API are configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d36c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONSISTENCY SUMMARY\n",
      "======================================================================\n",
      "              Approach Agreement Rate Avg Std Dev  N Reviews  Runs/Review\n",
      "      Zero-Shot Rubric          94.0%       0.035         50            3\n",
      "     Few-Shot Examples          96.0%       0.023         50            3\n",
      "Structured Constraints          96.0%       0.023         50            3\n",
      "======================================================================\n",
      "\n",
      "Note:\n",
      "  - Agreement Rate: % of reviews where all runs gave same prediction\n",
      "  - Avg Std Dev: average standard deviation of predictions (lower = more consistent)\n"
     ]
    }
   ],
   "source": [
    "if consistency_results:\n",
    "    cons_data = []\n",
    "    for cr in consistency_results:\n",
    "        cons_data.append({\n",
    "            \"Approach\": cr[\"approach\"],\n",
    "            \"Agreement Rate\": f\"{cr['agreement_rate']:.1%}\",\n",
    "            \"Avg Std Dev\": f\"{cr['avg_std']:.3f}\",\n",
    "            \"N Reviews\": cr[\"n_reviews\"],\n",
    "            \"Runs/Review\": cr[\"num_runs\"]\n",
    "        })\n",
    "    \n",
    "    cons_df = pd.DataFrame(cons_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONSISTENCY SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(cons_df.to_string(index=False))\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNote:\")\n",
    "    print(\"  - Agreement Rate: % of reviews where all runs gave same prediction\")\n",
    "    print(\"  - Avg Std Dev: average standard deviation of predictions (lower = more consistent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606eb2b",
   "metadata": {},
   "source": [
    "## 11. Discussion & Analysis\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "Based on the evaluation results:\n",
    "\n",
    "1. **Accuracy Trade-offs**\n",
    "   - Zero-Shot Rubric: Provides explicit criteria but may miss nuanced cases\n",
    "   - Few-Shot Examples: Context from real examples can improve edge case handling\n",
    "   - Structured Constraints: Strict formatting may trade accuracy for reliability\n",
    "\n",
    "2. **JSON Validity**\n",
    "   - Stricter prompts (Approach 3) tend to produce more valid JSON\n",
    "   - Few-shot examples can sometimes cause the model to deviate from format\n",
    "   - Temperature setting (0.2) helps maintain consistent output structure\n",
    "\n",
    "3. **Consistency/Reliability**\n",
    "   - Lower temperature (0.2) improves prediction consistency\n",
    "   - Agreement rate indicates how deterministic each approach is\n",
    "   - Production systems should prefer approaches with higher agreement rates\n",
    "\n",
    "4. **Latency Considerations**\n",
    "   - Few-shot prompts are longer → higher latency and cost\n",
    "   - Zero-shot approaches are most efficient for high-volume scenarios\n",
    "\n",
    "### Prompt Evolution Notes\n",
    "\n",
    "Key prompt engineering decisions:\n",
    "- Used triple quotes to clearly delimit review text\n",
    "- Explicit \"ONLY JSON\" instruction to minimize extra text\n",
    "- Word limits in explanation to control output length\n",
    "- Rating criteria in rubric approach for transparent reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cc91a",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155882c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: outputs/task1_comparison_20260109_181021.csv\n",
      "✓ Saved: outputs/task1_predictions_20260109_181021.csv\n",
      "✓ Saved: outputs/task1_consistency_20260109_181021.csv\n",
      "\n",
      "All outputs saved to: /Users/harshkanani/Desktop/fyndAssignment/notebooks/outputs\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if results:\n",
    "    comparison_path = OUTPUT_DIR / f\"task1_comparison_{timestamp}.csv\"\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    print(f\"✓ Saved: {comparison_path}\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    for result in results:\n",
    "        for pred in result[\"predictions\"]:\n",
    "            all_predictions.append({\n",
    "                \"approach\": result[\"approach\"],\n",
    "                **pred\n",
    "            })\n",
    "    \n",
    "    predictions_path = OUTPUT_DIR / f\"task1_predictions_{timestamp}.csv\"\n",
    "    pd.DataFrame(all_predictions).to_csv(predictions_path, index=False)\n",
    "    print(f\"✓ Saved: {predictions_path}\")\n",
    "\n",
    "if consistency_results:\n",
    "    consistency_path = OUTPUT_DIR / f\"task1_consistency_{timestamp}.csv\"\n",
    "    cons_df.to_csv(consistency_path, index=False)\n",
    "    print(f\"✓ Saved: {consistency_path}\")\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b3522",
   "metadata": {},
   "source": [
    "## 13. Conclusion\n",
    "\n",
    "This notebook evaluates three prompting approaches for predicting Yelp review star ratings:\n",
    "\n",
    "| Approach | Strengths | Weaknesses |\n",
    "|----------|-----------|------------|\n",
    "| **Zero-Shot Rubric** | Clear criteria, efficient | May miss nuances |\n",
    "| **Few-Shot Examples** | Real-world context | Longer prompts, higher cost |\n",
    "| **Structured Constraints** | Reliable JSON output | Less flexible reasoning |\n",
    "\n",
    "**Key Findings:**\n",
    "- All approaches use the standardized output schema: `{\"predicted_stars\": N, \"explanation\": \"...\"}`\n",
    "- Low temperature (0.2) significantly improves consistency and JSON validity\n",
    "- Trade-off exists between accuracy and JSON reliability\n",
    "\n",
    "**Metrics Reported:**\n",
    "- Accuracy (over all samples, and over valid JSON only)\n",
    "- JSON validity rate\n",
    "- Consistency (agreement rate, std dev)\n",
    "- Latency\n",
    "\n",
    "---\n",
    "\n",
    "*Fynd AI Intern Assessment - Task 1: Rating Prediction*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
